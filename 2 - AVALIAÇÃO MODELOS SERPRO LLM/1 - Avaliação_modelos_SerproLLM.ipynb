{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb2f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import datetime, date  # Importando as classes datetime e date corretamente\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "#Configurando certificado digital\n",
    "certificados_serpro = \"ca-pro.pem\"\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certificados_serpro\n",
    "os.environ[\"SSL_CERT_FILE\"] = certificados_serpro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8832477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname=\"PROCESSOS\",\n",
    "        user=\"postgres\",\n",
    "        password=\"\",\n",
    "        host=\"\",\n",
    "        port=\"5432\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c55834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "data = {\"grant_type\":\"client_credentials\"}\n",
    "url = \"\"\n",
    "result = requests.request('POST',url, data=data, auth=(client_id,client_secret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63e350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis globais para armazenar o token e o horário em que foi gerado\n",
    "token = None\n",
    "last_token_time = 0\n",
    "\n",
    "# Função para obter o token da API\n",
    "def get_token():\n",
    "    client_id = ''\n",
    "    client_secret = ''\n",
    "    result = requests.request('POST', \n",
    "        \"...\", \n",
    "        data={\"grant_type\":\"client_credentials\"}, \n",
    "        auth=(client_id, client_secret))\n",
    "\n",
    "    if result.ok:\n",
    "        return result.json()['access_token']\n",
    "    else:\n",
    "        raise Exception(\"Erro ao obter o token\")\n",
    "\n",
    "# Função para verificar se o token está expirado (renova se necessário)\n",
    "def get_valid_token():\n",
    "    global token, last_token_time\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Se o token não existe ou já passaram mais de 20 minutos, obtemos um novo\n",
    "    if token is None or (current_time - last_token_time) > 20 * 60:  # 20 minutos em segundos\n",
    "        token = get_token()\n",
    "        last_token_time = current_time\n",
    "        print(\"Novo token obtido.\")\n",
    "\n",
    "    return token\n",
    "\n",
    "# Função para invocar o LLM\n",
    "def invoke(prompt, modelo,token, temperature=0, max_tokens=10000, stream=False):\n",
    "    # Obtém o token válido (renova se necessário)\n",
    "    \n",
    "\n",
    "    payload_data = {\n",
    "        \"model\": modelo,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "\n",
    "    result = requests.request(\"POST\", \n",
    "        '...', \n",
    "        data=json.dumps(payload_data), \n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\", \n",
    "            \"Content-Type\":\"application/json\"})\n",
    "\n",
    "    if result.ok:\n",
    "        res = result.json()\n",
    "        resposta = res[\"choices\"][0][\"message\"]['content']\n",
    "        return resposta\n",
    "    else:\n",
    "        print(result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d16d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que a função get_token já esteja definida\n",
    "token = get_token()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c9c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"codestral-22b\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"mistralai/Codestral-22B-v0.1\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 72080,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-4b07584c66c14b84bdcd979ec745c6e4\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Codestral 22B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"deepseek-r1-distill-qwen-14b\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 131072,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-00a9f5b450824549a18c4acf31bd9e8c\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"DeepSeek R1 Distill Qwen 14B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"gemma-2-9b-it\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"google/gemma-2-9b-it\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 8192,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-2e85feaa9c6b41e887237d90c652da7b\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Gemma 2 9B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"gemma-3-4b-it\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"google/gemma-3-4b-it\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 131072,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-3a9005433574457f888217d29fb24296\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Gemma 3 4B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"llama-3.1-8B-instruct\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 124928,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-17bfd021ff484701a6771bf20cd28aef\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"LLaMa 3.1 8B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"mistral-nemo-instruct\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 46080,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-e903517d33154811b3a75b73d1892632\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Mistral NeMo\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"pixtral-12b\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"mistralai/Pixtral-12B-2409\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 72080,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-152f8eacb2c0438b8bfbd10eee91ed29\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Pixtral 12B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"all-minilm-l6-v2\",\n",
      "    \"stats\": {\n",
      "        \"queue_fraction\": 0.0,\n",
      "        \"queue_absolute\": 0,\n",
      "        \"results_pending\": 0,\n",
      "        \"batch_size\": 32\n",
      "    },\n",
      "    \"object\": \"model\",\n",
      "    \"owned_by\": \"infinity\",\n",
      "    \"created\": 1745856871,\n",
      "    \"backend\": \"torch\",\n",
      "    \"capabilities\": [\n",
      "        \"embed\"\n",
      "    ],\n",
      "    \"model_type\": \"EMBEDDINGS\",\n",
      "    \"model_label\": \"all minilm l6 v2\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"multilingual-e5-large\",\n",
      "    \"stats\": {\n",
      "        \"queue_fraction\": 0.0,\n",
      "        \"queue_absolute\": 0,\n",
      "        \"results_pending\": 0,\n",
      "        \"batch_size\": 32\n",
      "    },\n",
      "    \"object\": \"model\",\n",
      "    \"owned_by\": \"infinity\",\n",
      "    \"created\": 1745856871,\n",
      "    \"backend\": \"torch\",\n",
      "    \"capabilities\": [\n",
      "        \"embed\"\n",
      "    ],\n",
      "    \"model_type\": \"EMBEDDINGS\",\n",
      "    \"model_label\": \"multilingual e5 large\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"bge-reranker-v2-m3\",\n",
      "    \"stats\": {\n",
      "        \"queue_fraction\": 0.0,\n",
      "        \"queue_absolute\": 0,\n",
      "        \"results_pending\": 0,\n",
      "        \"batch_size\": 32\n",
      "    },\n",
      "    \"object\": \"model\",\n",
      "    \"owned_by\": \"infinity\",\n",
      "    \"created\": 1745856871,\n",
      "    \"backend\": \"torch\",\n",
      "    \"capabilities\": [\n",
      "        \"rerank\"\n",
      "    ],\n",
      "    \"model_type\": \"RERANK\",\n",
      "    \"model_label\": \"bge reranker v2 m3\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Supondo que a função get_token já esteja definida\n",
    "token = get_token()\n",
    "\n",
    "def listarModelos():\n",
    "    url = \"...\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    result = requests.request(\"GET\", url, headers=headers)\n",
    "    if result.ok:\n",
    "        res = result.json()\n",
    "        return res['data']\n",
    "    else:\n",
    "        print(\"Nenhum modelo encontrado\")\n",
    "\n",
    "def print_modelos(modelos):\n",
    "    for modelo in modelos:\n",
    "        print(json.dumps(modelo, indent=4, ensure_ascii=False))\n",
    "\n",
    "modelos = listarModelos()\n",
    "if modelos:\n",
    "    print_modelos(modelos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b437d",
   "metadata": {},
   "source": [
    "### 📊 Comparativo de Modelos para Sumarização de Textos Jurídicos (100 a 20.000 tokens)\n",
    "\n",
    "| Modelo                         | Parâmetros   | Capacidade de Tokens | Adequação para Ementas com até 20.000 Tokens |\n",
    "|-------------------------------|--------------|-----------------------|---------------------------------------------|\n",
    "| **Codestral 22B**             | 22 bilhões   | 32.768                | ✅ Alta: Capaz de lidar com textos extensos com boa precisão linguística |\n",
    "| **LLaMa 3.1 8B**              | 8 bilhões    | 131.072               | ✅ Alta: Excelente para ementas extensas, robusto e versátil             |\n",
    "| **DeepSeek R1 Distill Qwen 14B** | 14 bilhões | 131.072               | ✅ Alta: Muito eficaz para textos jurídicos longos, boa generalização    |\n",
    "| **Pixtral 12B**               | 12 bilhões   | 72.080                | ✅ Alta: Ótimo equilíbrio entre capacidade e desempenho                  |\n",
    "| **Mistral NeMo**              | 12 bilhões         | 46.080                | ⚠️ Moderada: Suporta até o limite exigido, mas com menor margem         |\n",
    "| **Gemma 3 4B**                | 4 bilhões    | 131.072               | ⚠️ Moderada: Janela larga, mas modelo menos potente                     |\n",
    "| **Gemma 2 9B**                | 9 bilhões    | 4.096                 | ❌ Baixa: Insuficiente para textos boa parte das ementas                          |\n",
    "| **all-minilm-l6-v2**          | N/A          | N/A                   | ❌ Baixa: Embedding model, não realiza sumarização                      |\n",
    "| **multilingual-e5-large**     | N/A          | N/A                   | ❌ Baixa: Focado em vetorização, não geração                            |\n",
    "| **bge-reranker-v2-m3**        | N/A          | N/A                   | ❌ Baixa: Especializado em reranking, não sumariza textos               |\n",
    "\n",
    "> Testar **LLaMa 3.1 8B**, **DeepSeek R1**, **Codestral 22B** ou **Pixtral 12B**. Estes suportam textos extensos com precisão, verificar qual melhor sumariza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12c505",
   "metadata": {},
   "source": [
    "## Selecionando as amostras de 200 processos\n",
    "\n",
    "### 🎯 Estratégia de Seleção da Amostra de Processos\n",
    "\n",
    "Para análises e testes de sumarização automática de ementas jurídicas, foi construída uma amostra balanceada contendo **200 processos** a partir da base original. O critério de seleção considerou a **quantidade de tokens da ementa** (`qtd_tokens_ementa`), respeitando os seguintes passos:\n",
    "\n",
    "#### 🧹 Pré-processamento\n",
    "- Foram considerados apenas os processos cuja ementa possui **100 tokens ou mais**.\n",
    "\n",
    "#### 📊 Estratificação por Faixas de Comprimento\n",
    "Com base na distribuição estatística dos tokens, foram calculados os seguintes quartis:\n",
    "- **Q1 (25%)**: primeiro quartil\n",
    "- **Mediana (50%)**\n",
    "- **Q3 (75%)**\n",
    "- **Limite Superior**: definido como `Q3 + 1.5 × IQR`, onde `IQR = Q3 - Q1`\n",
    "\n",
    "A partir disso, a amostra foi estratificada em **quatro faixas de tamanho**, com **50 processos selecionados aleatoriamente** em cada uma delas:\n",
    "\n",
    "| Faixa                      | Critério de Seleção                        | Quantidade |\n",
    "|----------------------------|--------------------------------------------|------------|\n",
    "| Faixa 1 – Curta            | 100 tokens até antes de Q1                 | 50         |\n",
    "| Faixa 2 – Média-curta      | Q1 até antes da Mediana                   | 50         |\n",
    "| Faixa 3 – Média-longa      | Mediana até antes de Q3                   | 50         |\n",
    "| Faixa 4 – Longa            | Q3 até o limite superior (Q3 + 1.5 × IQR) | 50         |\n",
    "\n",
    "#### 📦 Resultado Final\n",
    "- Total de processos selecionados: **200**\n",
    "- Campos considerados: `numero_processo_tribunal`, `ementa_completa`, `qtd_tokens_ementa`\n",
    "\n",
    "Essa amostra reflete  a variação real no tamanho das ementas, permitindo a avaliação de modelos de linguagem para diferentes níveis de complexidade textual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df73c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos\n",
    "modelos = ['codestral-22b', 'deepseek-r1-distill-qwen-14b', 'llama-3.1-8B-instruct', 'pixtral-12b']\n",
    "\n",
    "# Carregar dados com pelo menos 100 tokens\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT numero_processo_tribunal, ementa_completa, qtd_tokens_ementa\n",
    "    FROM processos\n",
    "    WHERE qtd_tokens_ementa IS NOT NULL AND qtd_tokens_ementa >= 100\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "\n",
    "# Calcular quartis\n",
    "q1 = np.percentile(df['qtd_tokens_ementa'], 25)\n",
    "q2 = np.percentile(df['qtd_tokens_ementa'], 50)\n",
    "q3 = np.percentile(df['qtd_tokens_ementa'], 75)\n",
    "iqr = q3 - q1\n",
    "limite_sup = q3 + 1.5 * iqr\n",
    "\n",
    "# Selecionar 50 amostras de cada faixa\n",
    "amostra_1 = df[df['qtd_tokens_ementa'] < q1].sample(n=50, random_state=42)\n",
    "amostra_2 = df[(df['qtd_tokens_ementa'] >= q1) & (df['qtd_tokens_ementa'] < q2)].sample(n=50, random_state=42)\n",
    "amostra_3 = df[(df['qtd_tokens_ementa'] >= q2) & (df['qtd_tokens_ementa'] < q3)].sample(n=50, random_state=42)\n",
    "amostra_4 = df[(df['qtd_tokens_ementa'] >= q3) & (df['qtd_tokens_ementa'] <= limite_sup)].sample(n=50, random_state=42)\n",
    "\n",
    "# Unir amostras\n",
    "amostra = pd.concat([amostra_1, amostra_2, amostra_3, amostra_4], ignore_index=True)\n",
    "\n",
    "# Replicar cada registro 4 vezes, um para cada modelo\n",
    "amostras_replicadas = pd.DataFrame([\n",
    "    {\n",
    "        'numero_processo_tribunal': row['numero_processo_tribunal'],\n",
    "        'ementa_completa': row['ementa_completa'],\n",
    "        'qtd_tokens_ementa': row['qtd_tokens_ementa'],\n",
    "        'modelo': modelo\n",
    "    }\n",
    "    for _, row in amostra.iterrows()\n",
    "    for modelo in modelos\n",
    "])\n",
    "\n",
    "# Inserir no banco\n",
    "conn = get_connection()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for _, row in amostras_replicadas.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO avaliacoes_sumarizacao (\n",
    "            numero_processo_tribunal,\n",
    "            ementa_completa,\n",
    "            qtd_tokens_ementa,\n",
    "            modelo\n",
    "        ) VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (\n",
    "        row['numero_processo_tribunal'],\n",
    "        row['ementa_completa'],\n",
    "        int(row['qtd_tokens_ementa']),\n",
    "        row['modelo']\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"{len(amostras_replicadas)} registros inseridos com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873f51a",
   "metadata": {},
   "source": [
    "## 📌 Extração de Informações com Modelos de Linguagem (LLMs)\n",
    "\n",
    "Serão aplicados **quatro modelos de linguagem** para realizar a extração automática de informações jurídicas a partir de 200 processos selecionados. Os modelos utilizados são:\n",
    "\n",
    "- `codestral-22b`  \n",
    "- `deepseek-r1-distill-qwen-14b`  \n",
    "- `llama-3.1-8B-instruct`  \n",
    "- `pixtral-12b`\n",
    "\n",
    "Cada modelo será aplicado sobre os **200 processos**, resultando em um total de **800 avaliações geradas**.\n",
    "\n",
    "### 💾 Armazenamento dos Resultados\n",
    "\n",
    "As respostas geradas por cada LLM serão armazenadas na tabela `avaliacoes_sumarizacao`, no campo `resposta`.\n",
    "\n",
    "### 🧪 Avaliação Posterior\n",
    "\n",
    "As respostas serão posteriormente avaliadas em dois aspectos principais:\n",
    "\n",
    "1. **Formato:** Verificação da conformidade com a estrutura JSON esperada e com os domínios esperados (pergunta 1 a 4)  \n",
    "2. **Conteúdo:** Análise da correção e completude das informações extraídas, com base na ementa original. (perguntas 5 a 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371e206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montar_prompt(ementa):\n",
    "    prompt = f\"\"\"### INSTRUÇÕES ###\n",
    "Você atua como um assistente jurídico com expertise em tornar ementas judiciais complexas mais compreensíveis, utilizando linguagem clara e acessível. Sua atuação está alinhada com as diretrizes estabelecidas pelo Pacto Nacional do Judiciário pela Linguagem Simples, iniciativa do Conselho Nacional de Justiça (CNJ).\n",
    "Sua tarefa é transformar o texto em uma resposta clara, direta e fácil de entender por qualquer pessoa.\n",
    "Não introduza novas informações, hipóteses, argumentos, fundamentos jurídicos ou conclusões que não estejam expressamente contidos no texto fornecido e não realize inferências especulativas.\n",
    "Responda exclusivamente em português. Siga exatamente os formatos indicados.\n",
    "\n",
    "Com base na ementa fornecida, extraia os seguintes campos e retorne a resposta em formato JSON. \n",
    "Campos esperados:\n",
    "\n",
    "- \"descricao_caso\": Resuma o que aconteceu no processo, explicando os principais fatos e o que foi pedido. Não cite nomes de pessoas ou empresas. Use até 100 palavras.\n",
    "- \"questoes_em_discussao\": Liste os principais pontos discutidos no julgamento. Use até 200 palavras.\n",
    "- \"solucoes_propostas\": Explique, de forma resumida, como o problema foi analisado ou resolvido. Use até 200 palavras.\n",
    "- \"decisao\": Informe o resultado do processo. Use apenas uma das opções: \"Aceito\", \"Negado\" ou \"Em análise\".\n",
    "- \"tese\": Se houver, diga qual foi a ideia principal da decisão, como uma regra que pode ser usada em outros casos parecidos. Use até 100 palavras.\n",
    "- \"envolve_mei\": Informe apenas \"Sim\" ou \"Não\" se o processo fala de forma clara que um Microempreendedor Individual (MEI) é autor ou réu. Se for só uma citação genérica ou indireta, responda \"Não\".\n",
    "- \"mei_do_processo\": Se envolve_mei for \"Sim\", diga se o MEI é o \"Autor\" ou o \"Réu\". Se não der para saber claramente, escreva \"Ninguém\".\n",
    "\n",
    "### Exemplo de resposta esperada:\n",
    "```json\n",
    "{{\n",
    "  \"descricao_caso\": \"Um MEI entrou com uma ação contra o município alegando cobrança indevida de taxas que não estavam previstas em lei.\",\n",
    "  \"questoes_em_discussao\": \"Foi discutido se a cobrança tinha base legal e se era proporcional ao tipo de atividade do MEI.\",\n",
    "  \"solucoes_propostas\": \"O tribunal entendeu que a cobrança era indevida por falta de previsão legal e por não ser proporcional à atividade.\",\n",
    "  \"decisao\": \"Aceito\",\n",
    "  \"tese\": \"O município não pode cobrar taxas de MEI sem base legal e proporcionalidade.\",\n",
    "  \"envolve_mei\": \"Sim\",\n",
    "  \"mei_do_processo\": \"Autor\"\n",
    "}}\n",
    "---\n",
    "\n",
    "### EMENTA ###\n",
    "{ementa}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db3a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  \"descricao_caso\": \"Um recurso inominado foi interposto contra a cobrança de honorários advocatícios por uma pessoa física. O recurso questionou fatos não propostos no juízo anterior e a constitucionalidade da ação.\",\n",
      "  \"questoes_em_discussao\": \"As questões discutidas incluem a revelia de fatos não propostos, a análise de questões de ordem pública, a aplicabilidade do FONAJE, o prazo da resposta, a competência do juizado especial e a possibilidade de processamento de pretensão veiculada por pessoa jurídica.\",\n",
      "  \"solucoes_propostas\": \"O tribunal analisou as questões e reconheceu a incompetência do juizado especial apenas em relação à pessoa jurídica EFFTING ADVOGADOS ASSOCIADOS S/C. O recurso foi desprovido em parte.\",\n",
      "  \"decisao\": \"Parcialmente Aceito\",\n",
      "  \"tese\": \"O prazo da resposta que se inicia no dia da citação não se aplica ao sistema dos juizados especiais. A pessoa jurídica só pode processar pretensão veiculada por pessoa física em casos específicos.\",\n",
      "  \"envolve_mei\": \"Não\",\n",
      "  \"mei_do_processo\": \"Ninguém\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Modelos para avaliação\n",
    "modelos = [ 'deepseek-r1-distill-qwen-14b', 'llama-3.1-8B-instruct', 'pixtral-12b','codestral-22b',]\n",
    "token = get_token()\n",
    "\n",
    "# Função para conectar ao banco\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname=\"PROCESSOS\",\n",
    "        user=\"postgres\",\n",
    "        password=\"admin\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "\n",
    "# Loop por modelo\n",
    "for modelo in modelos:\n",
    "    # Buscar registros ainda não respondidos para o modelo atual\n",
    "    conn = get_connection()\n",
    "    query = \"\"\"\n",
    "        SELECT id, numero_processo_tribunal, ementa_completa, qtd_tokens_ementa\n",
    "        FROM avaliacoes_sumarizacao\n",
    "        WHERE modelo = %s AND resposta IS NULL\n",
    "    \"\"\"\n",
    "    df_modelo = pd.read_sql_query(query, conn, params=(modelo,))\n",
    "    conn.close()\n",
    "\n",
    "    total = len(df_modelo)\n",
    "    print(f\"Total de registros pendentes para {modelo}: {total}\")\n",
    "    \n",
    "    for i, row in df_modelo.iterrows():\n",
    "        print(f\"{i+1}/{total}\")\n",
    "        \n",
    "        \n",
    "        processo = row['numero_processo_tribunal']\n",
    "        ementa = row['ementa_completa']\n",
    "        qtd_tokens = row['qtd_tokens_ementa']\n",
    "        prompt = montar_prompt(ementa)\n",
    "        \n",
    "        print(f\"Modelo: {modelo} | Processo: {processo}\")\n",
    "        print(prompt)\n",
    "\n",
    "        try:\n",
    "            resposta = invoke(prompt, modelo, token)\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            resposta_crua = resposta  # manter original para debug ou fallback\n",
    "            if (modelo == 'deepseek-r1-distill-qwen-14b'):\n",
    "                # Tenta extrair bloco JSON entre delimitadores ```json ... ```\n",
    "                match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", resposta_crua, re.DOTALL)\n",
    "\n",
    "                if match:\n",
    "                    try:\n",
    "                        parsed = json.loads(match.group(1))  # valida e padroniza o JSON\n",
    "                        resposta = json.dumps(parsed, ensure_ascii=False)\n",
    "                    except Exception as e:\n",
    "                        resposta = f\"[ERRO ao parsear JSON: {e}]\\n\\n{resposta_crua}\"\n",
    "                else:\n",
    "                    resposta = resposta_crua  # usa a resposta como está (por exemplo, Codestral já responde direto)\n",
    "\n",
    "        except Exception as e:\n",
    "            resposta = f\"[ERRO: {e}]\"\n",
    "\n",
    "        print(resposta)\n",
    "\n",
    "        # Atualiza resposta no banco\n",
    "        conn = get_connection()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE avaliacoes_sumarizacao\n",
    "            SET resposta = %s\n",
    "            WHERE id = %s\n",
    "        \"\"\", (resposta, row['id']))\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13d5e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 respostas limpas e atualizadas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loreane\\AppData\\Local\\Temp\\ipykernel_13752\\3021378228.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(\"SELECT id, resposta FROM avaliacoes_sumarizacao WHERE resposta IS NOT NULL\", conn)\n"
     ]
    }
   ],
   "source": [
    "# EM algumas respostas ele esta colocando os termos ```json e ```, pedi para retirar\n",
    "\n",
    "# Conectar ao banco e carregar os dados com resposta não nula\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(\"SELECT id, resposta FROM avaliacoes_sumarizacao WHERE resposta IS NOT NULL\", conn)\n",
    "\n",
    "# Limpa os delimitadores ``` e ```json\n",
    "def limpar_delimitadores(texto):\n",
    "    texto = re.sub(r\"```json\\s*\", \"\", texto, flags=re.IGNORECASE)\n",
    "    texto = re.sub(r\"```\", \"\", texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# Aplicar limpeza e armazenar atualizações\n",
    "updates = []\n",
    "for _, row in df.iterrows():\n",
    "    resposta_limpa = limpar_delimitadores(row['resposta'])\n",
    "    if resposta_limpa != row['resposta']:\n",
    "        updates.append((resposta_limpa, row['id']))\n",
    "\n",
    "# Atualizar banco de dados\n",
    "if updates:\n",
    "    cursor = conn.cursor()\n",
    "    for resposta_limpa, id_ in updates:\n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE avaliacoes_sumarizacao\n",
    "            SET resposta = %s\n",
    "            WHERE id = %s\n",
    "        \"\"\", (resposta_limpa, id_))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "conn.close()\n",
    "print(f\"{len(updates)} respostas limpas e atualizadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda3dad",
   "metadata": {},
   "source": [
    "# 🧪 Avaliação Automática das Respostas Geradas (Notas 1 a 4)\n",
    "\n",
    "Este script realiza a **validação automática das respostas JSON geradas pelos modelos LLM** e atribui **notas de 1 a 4** com base em critérios objetivos, para controle de qualidade da sumarização jurídica.\n",
    "\n",
    "## 🎯 Critérios de Avaliação\n",
    "\n",
    "| Nota | Critério                                                                                   | Valor Esperado     | Pontuação |\n",
    "|------|---------------------------------------------------------------------------------------------|--------------------|-----------|\n",
    "| 1    | O campo `resumo` contém um JSON bem formado (válido no `json.loads`)                        | JSON válido        | 1 ou 0    |\n",
    "| 2    | O campo `\"decisao\"` da resposta contém apenas `\"Aceito\"` ou `\"Negado\"`                     | \"Aceito\" / \"Negado\"| 1 ou 0    |\n",
    "| 3    | O campo `\"envolve_mei\"` contém apenas `\"Sim\"` ou `\"Não\"`                                    | \"Sim\" / \"Não\"      | 1 ou 0    |\n",
    "| 4    | O campo `\"mei_do_processo\"` contém apenas `\"Autor\"`, `\"Réu\"` ou `\"Ninguém\"`                | Válidos            | 1 ou 0    |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Identificação e Atualização no Banco\n",
    "\n",
    "- Cada entrada no banco de dados é identificada pela coluna `id` da tabela `avaliacoes_sumarizacao`.\n",
    "- O campo `resumo` contém a resposta textual retornada pelo modelo.\n",
    "- O script **lê o campo `resumo`**, avalia os critérios acima e **atualiza diretamente os campos `nota1` a `nota4`** no banco de dados para o respectivo registro.\n",
    "\n",
    "---\n",
    "\n",
    "✅ Isso permite que a avaliação seja incremental, reproduzível e utilizada em análises comparativas entre diferentes modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a06c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loreane\\AppData\\Local\\Temp\\ipykernel_13752\\3930452485.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(\"SELECT id, resposta FROM avaliacoes_sumarizacao\", conn)\n"
     ]
    }
   ],
   "source": [
    "def avaliar_resposta(resposta_json):\n",
    "    try:\n",
    "        dados = json.loads(resposta_json)\n",
    "        nota1 = 1\n",
    "        nota2 = 1 if dados.get(\"decisao\") in [\"Aceito\", \"Negado\",\"Em análise\"] else 0\n",
    "        nota3 = 1 if dados.get(\"envolve_mei\") in [\"Sim\", \"Não\"] else 0\n",
    "        nota4 = 1 if dados.get(\"mei_do_processo\") in [\"Autor\", \"Réu\", \"Ninguém\"] else 0\n",
    "    except Exception:\n",
    "        nota1 = nota2 = nota3 = nota4 = 0\n",
    "    return nota1, nota2, nota3, nota4\n",
    "\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(\"SELECT id, resposta FROM avaliacoes_sumarizacao\", conn)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    id_registro = row[\"id\"]\n",
    "    resposta = row[\"resposta\"]\n",
    "    nota1, nota2, nota3, nota4 = avaliar_resposta(resposta)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE avaliacoes_sumarizacao\n",
    "        SET nota1 = %s, nota2 = %s, nota3 = %s, nota4 = %s\n",
    "        WHERE id = %s\n",
    "    \"\"\", (nota1, nota2, nota3, nota4, id_registro))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e34c9f4",
   "metadata": {},
   "source": [
    "# 🤖 Avaliação das Respostas Geradas com GPT-3.5 (Notas 5 a 8)\n",
    "\n",
    "Este bloco realiza uma avaliação semântica dos campos retornados pelo modelo de linguagem, utilizando o **GPT-3.5 da OpenAI via API**. O objetivo é verificar se os campos da resposta JSON seguem corretamente os critérios esperados quanto ao conteúdo, clareza e aderência à instrução original.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Critérios Avaliados\n",
    "\n",
    "| Nota | Campo Avaliado         | Critério                                                                                     | Esperado |\n",
    "|------|------------------------|----------------------------------------------------------------------------------------------|----------|\n",
    "| 5    | `descricao_caso`       | O texto apresenta os **fatos principais do processo** e **o que foi pedido na ação**?        | 1 ou 0   |\n",
    "| 6    | `questoes_em_discussao`| O texto **resume os principais pontos discutidos** no julgamento?                           | 1 ou 0   |\n",
    "| 7    | `tese`                 | O texto apresenta uma **regra geral da decisão** que pode ser usada em casos semelhantes?   | 1 ou 0   |\n",
    "| 8    | `solucoes_propostas`   | O texto explica, de forma breve, **os fundamentos usados na decisão**, focando na solução? | 1 ou 0   |\n",
    "| 9    | `envolve_mei`   | O campo **\"envolve_mei\"** recebeu a resposta correta? | 1 ou 0   |\n",
    "| 10    | `mei_do_processo`   |O campo **\"mei_do_processo\"** recebeu a resposta correta? | 1 ou 0   |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Estratégia Técnica\n",
    "\n",
    "- Para cada campo, uma **pergunta específica** é feita ao modelo GPT-3.5.\n",
    "- O texto correspondente do campo é passado como **contexto** dentro do prompt.\n",
    "- O GPT deve responder **apenas com `\"1\"` ou `\"0\"`**, sem justificativa.\n",
    "- A temperatura é fixada em `0.0` para garantir consistência e reduzir variação de resposta.\n",
    "\n",
    "---\n",
    "\n",
    "✅ Esse método permite **validação automatizada com interpretação semântica**, sem depender apenas de regras fixas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "524df27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Avaliação concluída.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname=\"PROCESSOS\",\n",
    "        user=\"postgres\",\n",
    "        password=\"admin\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "\n",
    "def avaliar_campos_com_gpt35(ementa, resposta_json):\n",
    "    prompt = f\"\"\"\n",
    "Você é um criterioso avaliador de respostas geradas por um modelo de linguagem para ementas judiciais.\n",
    "\n",
    "Receberá a ementa original e a resposta em JSON gerada por outro modelo. Sua tarefa é avaliar se cada campo da resposta está correto, comparando com a ementa.\n",
    "\n",
    "Avalie e retorne APENAS este JSON com \"1\" (correto) ou \"0\" (incorreto) para cada campo:\n",
    "\n",
    "{{\n",
    "  \"nota5\": ...,\n",
    "  \"nota6\": ...,\n",
    "  \"nota7\": ...,\n",
    "  \"nota8\": ...,\n",
    "  \"nota9\": ...,\n",
    "  \"nota10\": ...\n",
    "}}\n",
    "\n",
    "Critérios:\n",
    "- \"nota5\": O campo \"descricao_caso\" apresenta os fatos principais do processo e o que foi pedido?\n",
    "- \"nota6\": O campo \"questoes_em_discussao\" resume os pontos centrais discutidos no julgamento?\n",
    "- \"nota7\": O campo \"tese\" traz uma regra geral da decisão ou informa que não foi possível extrair a tese?\n",
    "- \"nota8\": O campo \"solucoes_propostas\" explica os fundamentos usados com foco na solução do problema?\n",
    "- \"nota9\": O campo \"envolve_mei\" recebeu a resposta correta?\n",
    "- \"nota10\": O campo \"mei_do_processo\" recebeu a resposta correta?\n",
    "\n",
    "### EMENTA ###\n",
    "{ementa}\n",
    "\n",
    "### RESPOSTA ###\n",
    "{resposta_json}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(prompt)\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        resultado = response.choices[0].message.content.strip()\n",
    "        print(\"🔍 Resposta da API:\", resultado)\n",
    "        notas = json.loads(resultado)\n",
    "        return (\n",
    "            int(notas.get(\"nota5\", 0)),\n",
    "            int(notas.get(\"nota6\", 0)),\n",
    "            int(notas.get(\"nota7\", 0)),\n",
    "            int(notas.get(\"nota8\", 0)),            \n",
    "            int(notas.get(\"nota9\", 0)),            \n",
    "            int(notas.get(\"nota10\", 0))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao avaliar: {e}\")\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "# Conectar e carregar os dados\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(\"SELECT id, ementa_completa, resposta AS resposta FROM avaliacoes_sumarizacao where nota5 is null\", conn)\n",
    "\n",
    "# Avaliar e atualizar no banco\n",
    "total = len(df)\n",
    "for idx, row in df.iterrows():\n",
    "    id_registro = row[\"id\"]\n",
    "    ementa = row[\"ementa_completa\"]\n",
    "    resposta_json = row[\"resposta\"]\n",
    "\n",
    "    print(f\"\\n Avaliando processo {idx + 1} de {total} (ID: {id_registro})\")\n",
    "    nota5, nota6, nota7, nota8, nota9, nota10 = avaliar_campos_com_gpt35(ementa, resposta_json)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE avaliacoes_sumarizacao\n",
    "        SET nota5 = %s, nota6 = %s, nota7 = %s, nota8 = %s, nota9 = %s, nota10 = %s\n",
    "        WHERE id = %s\n",
    "    \"\"\", (nota5, nota6, nota7, nota8, nota9, nota10, id_registro))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    clear_output(wait=True)\n",
    "conn.close()\n",
    "print(\"\\n✅ Avaliação concluída.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2dc2c",
   "metadata": {},
   "source": [
    "## RESULTADO\n",
    "\n",
    "## 🎯 Critérios de Avaliação\n",
    "\n",
    "| Nota | Critério                                                                                   | Valor Esperado     | Pontuação |\n",
    "|------|---------------------------------------------------------------------------------------------|--------------------|-----------|\n",
    "| 1    | O campo `resumo` contém um JSON bem formado (válido no `json.loads`)                        | JSON válido        | 1 ou 0    |\n",
    "| 2    | O campo `\"decisao\"` da resposta contém apenas `\"Aceito\"` ou `\"Negado\"`                     | \"Aceito\" / \"Negado\"| 1 ou 0    |\n",
    "| 3    | O campo `\"envolve_mei\"` contém apenas `\"Sim\"` ou `\"Não\"`                                    | \"Sim\" / \"Não\"      | 1 ou 0    |\n",
    "| 4    | O campo `\"mei_do_processo\"` contém apenas `\"Autor\"`, `\"Réu\"` ou `\"Ninguém\"`                | `\"Autor\"`, `\"Réu\"` ou `\"Ninguém\"`            | 1 ou 0    |\n",
    "| 5    | O campo `\"descricao_caso\"` apresenta os **fatos principais do processo** e **o que foi pedido na ação** | -                  | 1 ou 0    |\n",
    "| 6    | O campo `\"questoes_em_discussao\"` resume os **principais pontos discutidos** no julgamento | -                  | 1 ou 0    |\n",
    "| 7    | O campo `\"tese\"` apresenta uma **regra geral da decisão** aplicável a casos semelhantes     | -                  | 1 ou 0    |\n",
    "| 8    | O campo `\"solucoes_propostas\"` explica, de forma breve, **os fundamentos usados na decisão**, focando na solução | - | 1 ou 0    |\n",
    "| 9    | O campo `\"envolve_mei\"` recebeu a resposta correta (Sim/Não) de acordo com o conteúdo da ementa | -               | 1 ou 0    |\n",
    "| 10   | O campo `\"mei_do_processo\"` recebeu a resposta correta (Autor/Réu/Ninguém) conforme o processo | -               | 1 ou 0    |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4a94ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Desempenho dos Modelos por Categoria:\n",
      "                         modelo  nota_formato  nota_conteudo  nota_total\n",
      "3                   pixtral-12b           792           1176        1968\n",
      "0                 codestral-22b           776           1170        1946\n",
      "1  deepseek-r1-distill-qwen-14b           719           1146        1865\n",
      "2         llama-3.1-8B-instruct           799           1050        1849\n",
      "\n",
      "📋 Detalhamento por nota:\n",
      "                         modelo  soma_nota1  soma_nota2  soma_nota3  \\\n",
      "0                 codestral-22b         200         183         200   \n",
      "1  deepseek-r1-distill-qwen-14b         200         193         177   \n",
      "2         llama-3.1-8B-instruct         200         200         200   \n",
      "3                   pixtral-12b         200         194         200   \n",
      "\n",
      "   soma_nota4  soma_nota5  soma_nota6  soma_nota7  soma_nota8  soma_nota9  \\\n",
      "0         193         199         199         195         194         198   \n",
      "1         149         197         197         198         196         183   \n",
      "2         199         179         178         176         176         178   \n",
      "3         198         198         198         197         198         194   \n",
      "\n",
      "   soma_nota10  nota_formato  nota_conteudo  nota_total  \n",
      "0          185           776           1170        1946  \n",
      "1          175           719           1146        1865  \n",
      "2          163           799           1050        1849  \n",
      "3          191           792           1176        1968  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loreane\\AppData\\Local\\Temp\\ipykernel_13752\\2057984196.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "# Query base\n",
    "query = \"\"\"\n",
    "    SELECT modelo,\n",
    "           SUM(nota1) AS soma_nota1,\n",
    "           SUM(nota2) AS soma_nota2,\n",
    "           SUM(nota3) AS soma_nota3,\n",
    "           SUM(nota4) AS soma_nota4,\n",
    "           SUM(nota5) AS soma_nota5,\n",
    "           SUM(nota6) AS soma_nota6,\n",
    "           SUM(nota7) AS soma_nota7,\n",
    "           SUM(nota8) AS soma_nota8,\n",
    "           SUM(nota9) AS soma_nota9,\n",
    "           SUM(nota10) AS soma_nota10\n",
    "    FROM avaliacoes_sumarizacao\n",
    "    GROUP BY modelo\n",
    "    ORDER BY modelo\n",
    "\"\"\"\n",
    "\n",
    "# Carregar dados\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Agrupamentos\n",
    "df['nota_formato'] = df[['soma_nota1', 'soma_nota2', 'soma_nota3', 'soma_nota4']].sum(axis=1)\n",
    "df['nota_conteudo'] = df[['soma_nota5', 'soma_nota6', 'soma_nota7', 'soma_nota8', 'soma_nota9', 'soma_nota10']].sum(axis=1)\n",
    "df['nota_total'] = df['nota_formato'] + df['nota_conteudo']\n",
    "\n",
    "# Exibir resumo geral\n",
    "print(\"📊 Desempenho dos Modelos por Categoria:\")\n",
    "print(df[['modelo', 'nota_formato', 'nota_conteudo', 'nota_total']].sort_values(by='nota_total', ascending=False))\n",
    "\n",
    "# Exibir detalhamento completo (nota a nota)\n",
    "print(\"\\n📋 Detalhamento por nota:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94d960c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>soma_nota1</th>\n",
       "      <th>soma_nota2</th>\n",
       "      <th>soma_nota3</th>\n",
       "      <th>soma_nota4</th>\n",
       "      <th>soma_nota5</th>\n",
       "      <th>soma_nota6</th>\n",
       "      <th>soma_nota7</th>\n",
       "      <th>soma_nota8</th>\n",
       "      <th>soma_nota9</th>\n",
       "      <th>soma_nota10</th>\n",
       "      <th>nota_formato</th>\n",
       "      <th>nota_conteudo</th>\n",
       "      <th>nota_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codestral-22b</td>\n",
       "      <td>200</td>\n",
       "      <td>183</td>\n",
       "      <td>200</td>\n",
       "      <td>193</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>198</td>\n",
       "      <td>185</td>\n",
       "      <td>776</td>\n",
       "      <td>1170</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek-r1-distill-qwen-14b</td>\n",
       "      <td>200</td>\n",
       "      <td>193</td>\n",
       "      <td>177</td>\n",
       "      <td>149</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>196</td>\n",
       "      <td>183</td>\n",
       "      <td>175</td>\n",
       "      <td>719</td>\n",
       "      <td>1146</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-3.1-8B-instruct</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>163</td>\n",
       "      <td>799</td>\n",
       "      <td>1050</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pixtral-12b</td>\n",
       "      <td>200</td>\n",
       "      <td>194</td>\n",
       "      <td>200</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>194</td>\n",
       "      <td>191</td>\n",
       "      <td>792</td>\n",
       "      <td>1176</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         modelo  soma_nota1  soma_nota2  soma_nota3  \\\n",
       "0                 codestral-22b         200         183         200   \n",
       "1  deepseek-r1-distill-qwen-14b         200         193         177   \n",
       "2         llama-3.1-8B-instruct         200         200         200   \n",
       "3                   pixtral-12b         200         194         200   \n",
       "\n",
       "   soma_nota4  soma_nota5  soma_nota6  soma_nota7  soma_nota8  soma_nota9  \\\n",
       "0         193         199         199         195         194         198   \n",
       "1         149         197         197         198         196         183   \n",
       "2         199         179         178         176         176         178   \n",
       "3         198         198         198         197         198         194   \n",
       "\n",
       "   soma_nota10  nota_formato  nota_conteudo  nota_total  \n",
       "0          185           776           1170        1946  \n",
       "1          175           719           1146        1865  \n",
       "2          163           799           1050        1849  \n",
       "3          191           792           1176        1968  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7ba62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
