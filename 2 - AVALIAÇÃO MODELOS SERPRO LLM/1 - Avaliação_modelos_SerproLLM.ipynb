{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb2f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import datetime, date  # Importando as classes datetime e date corretamente\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "#Configurando certificado digital\n",
    "certificados_serpro = \"ca-pro.pem\"\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certificados_serpro\n",
    "os.environ[\"SSL_CERT_FILE\"] = certificados_serpro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8832477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConexÃ£o\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname=\"PROCESSOS\",\n",
    "        user=\"postgres\",\n",
    "        password=\"\",\n",
    "        host=\"\",\n",
    "        port=\"5432\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c55834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "data = {\"grant_type\":\"client_credentials\"}\n",
    "url = \"\"\n",
    "result = requests.request('POST',url, data=data, auth=(client_id,client_secret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63e350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VariÃ¡veis globais para armazenar o token e o horÃ¡rio em que foi gerado\n",
    "token = None\n",
    "last_token_time = 0\n",
    "\n",
    "# FunÃ§Ã£o para obter o token da API\n",
    "def get_token():\n",
    "    client_id = ''\n",
    "    client_secret = ''\n",
    "    result = requests.request('POST', \n",
    "        \"...\", \n",
    "        data={\"grant_type\":\"client_credentials\"}, \n",
    "        auth=(client_id, client_secret))\n",
    "\n",
    "    if result.ok:\n",
    "        return result.json()['access_token']\n",
    "    else:\n",
    "        raise Exception(\"Erro ao obter o token\")\n",
    "\n",
    "# FunÃ§Ã£o para verificar se o token estÃ¡ expirado (renova se necessÃ¡rio)\n",
    "def get_valid_token():\n",
    "    global token, last_token_time\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Se o token nÃ£o existe ou jÃ¡ passaram mais de 20 minutos, obtemos um novo\n",
    "    if token is None or (current_time - last_token_time) > 20 * 60:  # 20 minutos em segundos\n",
    "        token = get_token()\n",
    "        last_token_time = current_time\n",
    "        print(\"Novo token obtido.\")\n",
    "\n",
    "    return token\n",
    "\n",
    "# FunÃ§Ã£o para invocar o LLM\n",
    "def invoke(prompt, modelo,token, temperature=0, max_tokens=10000, stream=False):\n",
    "    # ObtÃ©m o token vÃ¡lido (renova se necessÃ¡rio)\n",
    "    \n",
    "\n",
    "    payload_data = {\n",
    "        \"model\": modelo,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "\n",
    "    result = requests.request(\"POST\", \n",
    "        '...', \n",
    "        data=json.dumps(payload_data), \n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {token}\", \n",
    "            \"Content-Type\":\"application/json\"})\n",
    "\n",
    "    if result.ok:\n",
    "        res = result.json()\n",
    "        resposta = res[\"choices\"][0][\"message\"]['content']\n",
    "        return resposta\n",
    "    else:\n",
    "        print(result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d16d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que a funÃ§Ã£o get_token jÃ¡ esteja definida\n",
    "token = get_token()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c9c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"codestral-22b\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"mistralai/Codestral-22B-v0.1\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 72080,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-4b07584c66c14b84bdcd979ec745c6e4\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Codestral 22B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"deepseek-r1-distill-qwen-14b\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 131072,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-00a9f5b450824549a18c4acf31bd9e8c\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"DeepSeek R1 Distill Qwen 14B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"gemma-2-9b-it\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"google/gemma-2-9b-it\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 8192,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-2e85feaa9c6b41e887237d90c652da7b\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Gemma 2 9B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"gemma-3-4b-it\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"google/gemma-3-4b-it\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 131072,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-3a9005433574457f888217d29fb24296\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Gemma 3 4B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"llama-3.1-8B-instruct\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 124928,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-17bfd021ff484701a6771bf20cd28aef\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"LLaMa 3.1 8B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"mistral-nemo-instruct\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 46080,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-e903517d33154811b3a75b73d1892632\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Mistral NeMo\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"pixtral-12b\",\n",
      "    \"object\": \"model\",\n",
      "    \"created\": 1745856871,\n",
      "    \"owned_by\": \"vllm\",\n",
      "    \"root\": \"mistralai/Pixtral-12B-2409\",\n",
      "    \"parent\": null,\n",
      "    \"max_model_len\": 72080,\n",
      "    \"permission\": [\n",
      "        {\n",
      "            \"id\": \"modelperm-152f8eacb2c0438b8bfbd10eee91ed29\",\n",
      "            \"object\": \"model_permission\",\n",
      "            \"created\": 1745856871,\n",
      "            \"allow_create_engine\": false,\n",
      "            \"allow_sampling\": true,\n",
      "            \"allow_logprobs\": true,\n",
      "            \"allow_search_indices\": false,\n",
      "            \"allow_view\": true,\n",
      "            \"allow_fine_tuning\": false,\n",
      "            \"organization\": \"*\",\n",
      "            \"group\": null,\n",
      "            \"is_blocking\": false\n",
      "        }\n",
      "    ],\n",
      "    \"model_type\": \"LLM\",\n",
      "    \"model_label\": \"Pixtral 12B\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"all-minilm-l6-v2\",\n",
      "    \"stats\": {\n",
      "        \"queue_fraction\": 0.0,\n",
      "        \"queue_absolute\": 0,\n",
      "        \"results_pending\": 0,\n",
      "        \"batch_size\": 32\n",
      "    },\n",
      "    \"object\": \"model\",\n",
      "    \"owned_by\": \"infinity\",\n",
      "    \"created\": 1745856871,\n",
      "    \"backend\": \"torch\",\n",
      "    \"capabilities\": [\n",
      "        \"embed\"\n",
      "    ],\n",
      "    \"model_type\": \"EMBEDDINGS\",\n",
      "    \"model_label\": \"all minilm l6 v2\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"multilingual-e5-large\",\n",
      "    \"stats\": {\n",
      "        \"queue_fraction\": 0.0,\n",
      "        \"queue_absolute\": 0,\n",
      "        \"results_pending\": 0,\n",
      "        \"batch_size\": 32\n",
      "    },\n",
      "    \"object\": \"model\",\n",
      "    \"owned_by\": \"infinity\",\n",
      "    \"created\": 1745856871,\n",
      "    \"backend\": \"torch\",\n",
      "    \"capabilities\": [\n",
      "        \"embed\"\n",
      "    ],\n",
      "    \"model_type\": \"EMBEDDINGS\",\n",
      "    \"model_label\": \"multilingual e5 large\"\n",
      "}\n",
      "{\n",
      "    \"id\": \"bge-reranker-v2-m3\",\n",
      "    \"stats\": {\n",
      "        \"queue_fraction\": 0.0,\n",
      "        \"queue_absolute\": 0,\n",
      "        \"results_pending\": 0,\n",
      "        \"batch_size\": 32\n",
      "    },\n",
      "    \"object\": \"model\",\n",
      "    \"owned_by\": \"infinity\",\n",
      "    \"created\": 1745856871,\n",
      "    \"backend\": \"torch\",\n",
      "    \"capabilities\": [\n",
      "        \"rerank\"\n",
      "    ],\n",
      "    \"model_type\": \"RERANK\",\n",
      "    \"model_label\": \"bge reranker v2 m3\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Supondo que a funÃ§Ã£o get_token jÃ¡ esteja definida\n",
    "token = get_token()\n",
    "\n",
    "def listarModelos():\n",
    "    url = \"...\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    result = requests.request(\"GET\", url, headers=headers)\n",
    "    if result.ok:\n",
    "        res = result.json()\n",
    "        return res['data']\n",
    "    else:\n",
    "        print(\"Nenhum modelo encontrado\")\n",
    "\n",
    "def print_modelos(modelos):\n",
    "    for modelo in modelos:\n",
    "        print(json.dumps(modelo, indent=4, ensure_ascii=False))\n",
    "\n",
    "modelos = listarModelos()\n",
    "if modelos:\n",
    "    print_modelos(modelos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79b437d",
   "metadata": {},
   "source": [
    "### ðŸ“Š Comparativo de Modelos para SumarizaÃ§Ã£o de Textos JurÃ­dicos (100 a 20.000 tokens)\n",
    "\n",
    "| Modelo                         | ParÃ¢metros   | Capacidade de Tokens | AdequaÃ§Ã£o para Ementas com atÃ© 20.000 Tokens |\n",
    "|-------------------------------|--------------|-----------------------|---------------------------------------------|\n",
    "| **Codestral 22B**             | 22 bilhÃµes   | 32.768                | âœ… Alta: Capaz de lidar com textos extensos com boa precisÃ£o linguÃ­stica |\n",
    "| **LLaMa 3.1 8B**              | 8 bilhÃµes    | 131.072               | âœ… Alta: Excelente para ementas extensas, robusto e versÃ¡til             |\n",
    "| **DeepSeek R1 Distill Qwen 14B** | 14 bilhÃµes | 131.072               | âœ… Alta: Muito eficaz para textos jurÃ­dicos longos, boa generalizaÃ§Ã£o    |\n",
    "| **Pixtral 12B**               | 12 bilhÃµes   | 72.080                | âœ… Alta: Ã“timo equilÃ­brio entre capacidade e desempenho                  |\n",
    "| **Mistral NeMo**              | 12 bilhÃµes         | 46.080                | âš ï¸ Moderada: Suporta atÃ© o limite exigido, mas com menor margem         |\n",
    "| **Gemma 3 4B**                | 4 bilhÃµes    | 131.072               | âš ï¸ Moderada: Janela larga, mas modelo menos potente                     |\n",
    "| **Gemma 2 9B**                | 9 bilhÃµes    | 4.096                 | âŒ Baixa: Insuficiente para textos boa parte das ementas                          |\n",
    "| **all-minilm-l6-v2**          | N/A          | N/A                   | âŒ Baixa: Embedding model, nÃ£o realiza sumarizaÃ§Ã£o                      |\n",
    "| **multilingual-e5-large**     | N/A          | N/A                   | âŒ Baixa: Focado em vetorizaÃ§Ã£o, nÃ£o geraÃ§Ã£o                            |\n",
    "| **bge-reranker-v2-m3**        | N/A          | N/A                   | âŒ Baixa: Especializado em reranking, nÃ£o sumariza textos               |\n",
    "\n",
    "> Testar **LLaMa 3.1 8B**, **DeepSeek R1**, **Codestral 22B** ou **Pixtral 12B**. Estes suportam textos extensos com precisÃ£o, verificar qual melhor sumariza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12c505",
   "metadata": {},
   "source": [
    "## Selecionando as amostras de 200 processos\n",
    "\n",
    "### ðŸŽ¯ EstratÃ©gia de SeleÃ§Ã£o da Amostra de Processos\n",
    "\n",
    "Para anÃ¡lises e testes de sumarizaÃ§Ã£o automÃ¡tica de ementas jurÃ­dicas, foi construÃ­da uma amostra balanceada contendo **200 processos** a partir da base original. O critÃ©rio de seleÃ§Ã£o considerou a **quantidade de tokens da ementa** (`qtd_tokens_ementa`), respeitando os seguintes passos:\n",
    "\n",
    "#### ðŸ§¹ PrÃ©-processamento\n",
    "- Foram considerados apenas os processos cuja ementa possui **100 tokens ou mais**.\n",
    "\n",
    "#### ðŸ“Š EstratificaÃ§Ã£o por Faixas de Comprimento\n",
    "Com base na distribuiÃ§Ã£o estatÃ­stica dos tokens, foram calculados os seguintes quartis:\n",
    "- **Q1 (25%)**: primeiro quartil\n",
    "- **Mediana (50%)**\n",
    "- **Q3 (75%)**\n",
    "- **Limite Superior**: definido como `Q3 + 1.5 Ã— IQR`, onde `IQR = Q3 - Q1`\n",
    "\n",
    "A partir disso, a amostra foi estratificada em **quatro faixas de tamanho**, com **50 processos selecionados aleatoriamente** em cada uma delas:\n",
    "\n",
    "| Faixa                      | CritÃ©rio de SeleÃ§Ã£o                        | Quantidade |\n",
    "|----------------------------|--------------------------------------------|------------|\n",
    "| Faixa 1 â€“ Curta            | 100 tokens atÃ© antes de Q1                 | 50         |\n",
    "| Faixa 2 â€“ MÃ©dia-curta      | Q1 atÃ© antes da Mediana                   | 50         |\n",
    "| Faixa 3 â€“ MÃ©dia-longa      | Mediana atÃ© antes de Q3                   | 50         |\n",
    "| Faixa 4 â€“ Longa            | Q3 atÃ© o limite superior (Q3 + 1.5 Ã— IQR) | 50         |\n",
    "\n",
    "#### ðŸ“¦ Resultado Final\n",
    "- Total de processos selecionados: **200**\n",
    "- Campos considerados: `numero_processo_tribunal`, `ementa_completa`, `qtd_tokens_ementa`\n",
    "\n",
    "Essa amostra reflete  a variaÃ§Ã£o real no tamanho das ementas, permitindo a avaliaÃ§Ã£o de modelos de linguagem para diferentes nÃ­veis de complexidade textual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df73c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos\n",
    "modelos = ['codestral-22b', 'deepseek-r1-distill-qwen-14b', 'llama-3.1-8B-instruct', 'pixtral-12b']\n",
    "\n",
    "# Carregar dados com pelo menos 100 tokens\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT numero_processo_tribunal, ementa_completa, qtd_tokens_ementa\n",
    "    FROM processos\n",
    "    WHERE qtd_tokens_ementa IS NOT NULL AND qtd_tokens_ementa >= 100\n",
    "\"\"\", conn)\n",
    "conn.close()\n",
    "\n",
    "# Calcular quartis\n",
    "q1 = np.percentile(df['qtd_tokens_ementa'], 25)\n",
    "q2 = np.percentile(df['qtd_tokens_ementa'], 50)\n",
    "q3 = np.percentile(df['qtd_tokens_ementa'], 75)\n",
    "iqr = q3 - q1\n",
    "limite_sup = q3 + 1.5 * iqr\n",
    "\n",
    "# Selecionar 50 amostras de cada faixa\n",
    "amostra_1 = df[df['qtd_tokens_ementa'] < q1].sample(n=50, random_state=42)\n",
    "amostra_2 = df[(df['qtd_tokens_ementa'] >= q1) & (df['qtd_tokens_ementa'] < q2)].sample(n=50, random_state=42)\n",
    "amostra_3 = df[(df['qtd_tokens_ementa'] >= q2) & (df['qtd_tokens_ementa'] < q3)].sample(n=50, random_state=42)\n",
    "amostra_4 = df[(df['qtd_tokens_ementa'] >= q3) & (df['qtd_tokens_ementa'] <= limite_sup)].sample(n=50, random_state=42)\n",
    "\n",
    "# Unir amostras\n",
    "amostra = pd.concat([amostra_1, amostra_2, amostra_3, amostra_4], ignore_index=True)\n",
    "\n",
    "# Replicar cada registro 4 vezes, um para cada modelo\n",
    "amostras_replicadas = pd.DataFrame([\n",
    "    {\n",
    "        'numero_processo_tribunal': row['numero_processo_tribunal'],\n",
    "        'ementa_completa': row['ementa_completa'],\n",
    "        'qtd_tokens_ementa': row['qtd_tokens_ementa'],\n",
    "        'modelo': modelo\n",
    "    }\n",
    "    for _, row in amostra.iterrows()\n",
    "    for modelo in modelos\n",
    "])\n",
    "\n",
    "# Inserir no banco\n",
    "conn = get_connection()\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for _, row in amostras_replicadas.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO avaliacoes_sumarizacao (\n",
    "            numero_processo_tribunal,\n",
    "            ementa_completa,\n",
    "            qtd_tokens_ementa,\n",
    "            modelo\n",
    "        ) VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (\n",
    "        row['numero_processo_tribunal'],\n",
    "        row['ementa_completa'],\n",
    "        int(row['qtd_tokens_ementa']),\n",
    "        row['modelo']\n",
    "    ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(f\"{len(amostras_replicadas)} registros inseridos com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5873f51a",
   "metadata": {},
   "source": [
    "## ðŸ“Œ ExtraÃ§Ã£o de InformaÃ§Ãµes com Modelos de Linguagem (LLMs)\n",
    "\n",
    "SerÃ£o aplicados **quatro modelos de linguagem** para realizar a extraÃ§Ã£o automÃ¡tica de informaÃ§Ãµes jurÃ­dicas a partir de 200 processos selecionados. Os modelos utilizados sÃ£o:\n",
    "\n",
    "- `codestral-22b`  \n",
    "- `deepseek-r1-distill-qwen-14b`  \n",
    "- `llama-3.1-8B-instruct`  \n",
    "- `pixtral-12b`\n",
    "\n",
    "Cada modelo serÃ¡ aplicado sobre os **200 processos**, resultando em um total de **800 avaliaÃ§Ãµes geradas**.\n",
    "\n",
    "### ðŸ’¾ Armazenamento dos Resultados\n",
    "\n",
    "As respostas geradas por cada LLM serÃ£o armazenadas na tabela `avaliacoes_sumarizacao`, no campo `resposta`.\n",
    "\n",
    "### ðŸ§ª AvaliaÃ§Ã£o Posterior\n",
    "\n",
    "As respostas serÃ£o posteriormente avaliadas em dois aspectos principais:\n",
    "\n",
    "1. **Formato:** VerificaÃ§Ã£o da conformidade com a estrutura JSON esperada e com os domÃ­nios esperados (pergunta 1 a 4)  \n",
    "2. **ConteÃºdo:** AnÃ¡lise da correÃ§Ã£o e completude das informaÃ§Ãµes extraÃ­das, com base na ementa original. (perguntas 5 a 10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371e206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def montar_prompt(ementa):\n",
    "    prompt = f\"\"\"### INSTRUÃ‡Ã•ES ###\n",
    "VocÃª atua como um assistente jurÃ­dico com expertise em tornar ementas judiciais complexas mais compreensÃ­veis, utilizando linguagem clara e acessÃ­vel. Sua atuaÃ§Ã£o estÃ¡ alinhada com as diretrizes estabelecidas pelo Pacto Nacional do JudiciÃ¡rio pela Linguagem Simples, iniciativa do Conselho Nacional de JustiÃ§a (CNJ).\n",
    "Sua tarefa Ã© transformar o texto em uma resposta clara, direta e fÃ¡cil de entender por qualquer pessoa.\n",
    "NÃ£o introduza novas informaÃ§Ãµes, hipÃ³teses, argumentos, fundamentos jurÃ­dicos ou conclusÃµes que nÃ£o estejam expressamente contidos no texto fornecido e nÃ£o realize inferÃªncias especulativas.\n",
    "Responda exclusivamente em portuguÃªs. Siga exatamente os formatos indicados.\n",
    "\n",
    "Com base na ementa fornecida, extraia os seguintes campos e retorne a resposta em formato JSON. \n",
    "Campos esperados:\n",
    "\n",
    "- \"descricao_caso\": Resuma o que aconteceu no processo, explicando os principais fatos e o que foi pedido. NÃ£o cite nomes de pessoas ou empresas. Use atÃ© 100 palavras.\n",
    "- \"questoes_em_discussao\": Liste os principais pontos discutidos no julgamento. Use atÃ© 200 palavras.\n",
    "- \"solucoes_propostas\": Explique, de forma resumida, como o problema foi analisado ou resolvido. Use atÃ© 200 palavras.\n",
    "- \"decisao\": Informe o resultado do processo. Use apenas uma das opÃ§Ãµes: \"Aceito\", \"Negado\" ou \"Em anÃ¡lise\".\n",
    "- \"tese\": Se houver, diga qual foi a ideia principal da decisÃ£o, como uma regra que pode ser usada em outros casos parecidos. Use atÃ© 100 palavras.\n",
    "- \"envolve_mei\": Informe apenas \"Sim\" ou \"NÃ£o\" se o processo fala de forma clara que um Microempreendedor Individual (MEI) Ã© autor ou rÃ©u. Se for sÃ³ uma citaÃ§Ã£o genÃ©rica ou indireta, responda \"NÃ£o\".\n",
    "- \"mei_do_processo\": Se envolve_mei for \"Sim\", diga se o MEI Ã© o \"Autor\" ou o \"RÃ©u\". Se nÃ£o der para saber claramente, escreva \"NinguÃ©m\".\n",
    "\n",
    "### Exemplo de resposta esperada:\n",
    "```json\n",
    "{{\n",
    "  \"descricao_caso\": \"Um MEI entrou com uma aÃ§Ã£o contra o municÃ­pio alegando cobranÃ§a indevida de taxas que nÃ£o estavam previstas em lei.\",\n",
    "  \"questoes_em_discussao\": \"Foi discutido se a cobranÃ§a tinha base legal e se era proporcional ao tipo de atividade do MEI.\",\n",
    "  \"solucoes_propostas\": \"O tribunal entendeu que a cobranÃ§a era indevida por falta de previsÃ£o legal e por nÃ£o ser proporcional Ã  atividade.\",\n",
    "  \"decisao\": \"Aceito\",\n",
    "  \"tese\": \"O municÃ­pio nÃ£o pode cobrar taxas de MEI sem base legal e proporcionalidade.\",\n",
    "  \"envolve_mei\": \"Sim\",\n",
    "  \"mei_do_processo\": \"Autor\"\n",
    "}}\n",
    "---\n",
    "\n",
    "### EMENTA ###\n",
    "{ementa}\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8db3a3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  \"descricao_caso\": \"Um recurso inominado foi interposto contra a cobranÃ§a de honorÃ¡rios advocatÃ­cios por uma pessoa fÃ­sica. O recurso questionou fatos nÃ£o propostos no juÃ­zo anterior e a constitucionalidade da aÃ§Ã£o.\",\n",
      "  \"questoes_em_discussao\": \"As questÃµes discutidas incluem a revelia de fatos nÃ£o propostos, a anÃ¡lise de questÃµes de ordem pÃºblica, a aplicabilidade do FONAJE, o prazo da resposta, a competÃªncia do juizado especial e a possibilidade de processamento de pretensÃ£o veiculada por pessoa jurÃ­dica.\",\n",
      "  \"solucoes_propostas\": \"O tribunal analisou as questÃµes e reconheceu a incompetÃªncia do juizado especial apenas em relaÃ§Ã£o Ã  pessoa jurÃ­dica EFFTING ADVOGADOS ASSOCIADOS S/C. O recurso foi desprovido em parte.\",\n",
      "  \"decisao\": \"Parcialmente Aceito\",\n",
      "  \"tese\": \"O prazo da resposta que se inicia no dia da citaÃ§Ã£o nÃ£o se aplica ao sistema dos juizados especiais. A pessoa jurÃ­dica sÃ³ pode processar pretensÃ£o veiculada por pessoa fÃ­sica em casos especÃ­ficos.\",\n",
      "  \"envolve_mei\": \"NÃ£o\",\n",
      "  \"mei_do_processo\": \"NinguÃ©m\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Modelos para avaliaÃ§Ã£o\n",
    "modelos = [ 'deepseek-r1-distill-qwen-14b', 'llama-3.1-8B-instruct', 'pixtral-12b','codestral-22b',]\n",
    "token = get_token()\n",
    "\n",
    "# FunÃ§Ã£o para conectar ao banco\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname=\"PROCESSOS\",\n",
    "        user=\"postgres\",\n",
    "        password=\"admin\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "\n",
    "# Loop por modelo\n",
    "for modelo in modelos:\n",
    "    # Buscar registros ainda nÃ£o respondidos para o modelo atual\n",
    "    conn = get_connection()\n",
    "    query = \"\"\"\n",
    "        SELECT id, numero_processo_tribunal, ementa_completa, qtd_tokens_ementa\n",
    "        FROM avaliacoes_sumarizacao\n",
    "        WHERE modelo = %s AND resposta IS NULL\n",
    "    \"\"\"\n",
    "    df_modelo = pd.read_sql_query(query, conn, params=(modelo,))\n",
    "    conn.close()\n",
    "\n",
    "    total = len(df_modelo)\n",
    "    print(f\"Total de registros pendentes para {modelo}: {total}\")\n",
    "    \n",
    "    for i, row in df_modelo.iterrows():\n",
    "        print(f\"{i+1}/{total}\")\n",
    "        \n",
    "        \n",
    "        processo = row['numero_processo_tribunal']\n",
    "        ementa = row['ementa_completa']\n",
    "        qtd_tokens = row['qtd_tokens_ementa']\n",
    "        prompt = montar_prompt(ementa)\n",
    "        \n",
    "        print(f\"Modelo: {modelo} | Processo: {processo}\")\n",
    "        print(prompt)\n",
    "\n",
    "        try:\n",
    "            resposta = invoke(prompt, modelo, token)\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            resposta_crua = resposta  # manter original para debug ou fallback\n",
    "            if (modelo == 'deepseek-r1-distill-qwen-14b'):\n",
    "                # Tenta extrair bloco JSON entre delimitadores ```json ... ```\n",
    "                match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", resposta_crua, re.DOTALL)\n",
    "\n",
    "                if match:\n",
    "                    try:\n",
    "                        parsed = json.loads(match.group(1))  # valida e padroniza o JSON\n",
    "                        resposta = json.dumps(parsed, ensure_ascii=False)\n",
    "                    except Exception as e:\n",
    "                        resposta = f\"[ERRO ao parsear JSON: {e}]\\n\\n{resposta_crua}\"\n",
    "                else:\n",
    "                    resposta = resposta_crua  # usa a resposta como estÃ¡ (por exemplo, Codestral jÃ¡ responde direto)\n",
    "\n",
    "        except Exception as e:\n",
    "            resposta = f\"[ERRO: {e}]\"\n",
    "\n",
    "        print(resposta)\n",
    "\n",
    "        # Atualiza resposta no banco\n",
    "        conn = get_connection()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE avaliacoes_sumarizacao\n",
    "            SET resposta = %s\n",
    "            WHERE id = %s\n",
    "        \"\"\", (resposta, row['id']))\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13d5e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 respostas limpas e atualizadas.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loreane\\AppData\\Local\\Temp\\ipykernel_13752\\3021378228.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(\"SELECT id, resposta FROM avaliacoes_sumarizacao WHERE resposta IS NOT NULL\", conn)\n"
     ]
    }
   ],
   "source": [
    "# EM algumas respostas ele esta colocando os termos ```json e ```, pedi para retirar\n",
    "\n",
    "# Conectar ao banco e carregar os dados com resposta nÃ£o nula\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(\"SELECT id, resposta FROM avaliacoes_sumarizacao WHERE resposta IS NOT NULL\", conn)\n",
    "\n",
    "# Limpa os delimitadores ``` e ```json\n",
    "def limpar_delimitadores(texto):\n",
    "    texto = re.sub(r\"```json\\s*\", \"\", texto, flags=re.IGNORECASE)\n",
    "    texto = re.sub(r\"```\", \"\", texto)\n",
    "    return texto.strip()\n",
    "\n",
    "# Aplicar limpeza e armazenar atualizaÃ§Ãµes\n",
    "updates = []\n",
    "for _, row in df.iterrows():\n",
    "    resposta_limpa = limpar_delimitadores(row['resposta'])\n",
    "    if resposta_limpa != row['resposta']:\n",
    "        updates.append((resposta_limpa, row['id']))\n",
    "\n",
    "# Atualizar banco de dados\n",
    "if updates:\n",
    "    cursor = conn.cursor()\n",
    "    for resposta_limpa, id_ in updates:\n",
    "        cursor.execute(\"\"\"\n",
    "            UPDATE avaliacoes_sumarizacao\n",
    "            SET resposta = %s\n",
    "            WHERE id = %s\n",
    "        \"\"\", (resposta_limpa, id_))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "conn.close()\n",
    "print(f\"{len(updates)} respostas limpas e atualizadas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda3dad",
   "metadata": {},
   "source": [
    "# ðŸ§ª AvaliaÃ§Ã£o AutomÃ¡tica das Respostas Geradas (Notas 1 a 4)\n",
    "\n",
    "Este script realiza a **validaÃ§Ã£o automÃ¡tica das respostas JSON geradas pelos modelos LLM** e atribui **notas de 1 a 4** com base em critÃ©rios objetivos, para controle de qualidade da sumarizaÃ§Ã£o jurÃ­dica.\n",
    "\n",
    "## ðŸŽ¯ CritÃ©rios de AvaliaÃ§Ã£o\n",
    "\n",
    "| Nota | CritÃ©rio                                                                                   | Valor Esperado     | PontuaÃ§Ã£o |\n",
    "|------|---------------------------------------------------------------------------------------------|--------------------|-----------|\n",
    "| 1    | O campo `resumo` contÃ©m um JSON bem formado (vÃ¡lido no `json.loads`)                        | JSON vÃ¡lido        | 1 ou 0    |\n",
    "| 2    | O campo `\"decisao\"` da resposta contÃ©m apenas `\"Aceito\"` ou `\"Negado\"`                     | \"Aceito\" / \"Negado\"| 1 ou 0    |\n",
    "| 3    | O campo `\"envolve_mei\"` contÃ©m apenas `\"Sim\"` ou `\"NÃ£o\"`                                    | \"Sim\" / \"NÃ£o\"      | 1 ou 0    |\n",
    "| 4    | O campo `\"mei_do_processo\"` contÃ©m apenas `\"Autor\"`, `\"RÃ©u\"` ou `\"NinguÃ©m\"`                | VÃ¡lidos            | 1 ou 0    |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© IdentificaÃ§Ã£o e AtualizaÃ§Ã£o no Banco\n",
    "\n",
    "- Cada entrada no banco de dados Ã© identificada pela coluna `id` da tabela `avaliacoes_sumarizacao`.\n",
    "- O campo `resumo` contÃ©m a resposta textual retornada pelo modelo.\n",
    "- O script **lÃª o campo `resumo`**, avalia os critÃ©rios acima e **atualiza diretamente os campos `nota1` a `nota4`** no banco de dados para o respectivo registro.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… Isso permite que a avaliaÃ§Ã£o seja incremental, reproduzÃ­vel e utilizada em anÃ¡lises comparativas entre diferentes modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a06c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loreane\\AppData\\Local\\Temp\\ipykernel_13752\\3930452485.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(\"SELECT id, resposta FROM avaliacoes_sumarizacao\", conn)\n"
     ]
    }
   ],
   "source": [
    "def avaliar_resposta(resposta_json):\n",
    "    try:\n",
    "        dados = json.loads(resposta_json)\n",
    "        nota1 = 1\n",
    "        nota2 = 1 if dados.get(\"decisao\") in [\"Aceito\", \"Negado\",\"Em anÃ¡lise\"] else 0\n",
    "        nota3 = 1 if dados.get(\"envolve_mei\") in [\"Sim\", \"NÃ£o\"] else 0\n",
    "        nota4 = 1 if dados.get(\"mei_do_processo\") in [\"Autor\", \"RÃ©u\", \"NinguÃ©m\"] else 0\n",
    "    except Exception:\n",
    "        nota1 = nota2 = nota3 = nota4 = 0\n",
    "    return nota1, nota2, nota3, nota4\n",
    "\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(\"SELECT id, resposta FROM avaliacoes_sumarizacao\", conn)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    id_registro = row[\"id\"]\n",
    "    resposta = row[\"resposta\"]\n",
    "    nota1, nota2, nota3, nota4 = avaliar_resposta(resposta)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE avaliacoes_sumarizacao\n",
    "        SET nota1 = %s, nota2 = %s, nota3 = %s, nota4 = %s\n",
    "        WHERE id = %s\n",
    "    \"\"\", (nota1, nota2, nota3, nota4, id_registro))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e34c9f4",
   "metadata": {},
   "source": [
    "# ðŸ¤– AvaliaÃ§Ã£o das Respostas Geradas com GPT-3.5 (Notas 5 a 8)\n",
    "\n",
    "Este bloco realiza uma avaliaÃ§Ã£o semÃ¢ntica dos campos retornados pelo modelo de linguagem, utilizando o **GPT-3.5 da OpenAI via API**. O objetivo Ã© verificar se os campos da resposta JSON seguem corretamente os critÃ©rios esperados quanto ao conteÃºdo, clareza e aderÃªncia Ã  instruÃ§Ã£o original.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª CritÃ©rios Avaliados\n",
    "\n",
    "| Nota | Campo Avaliado         | CritÃ©rio                                                                                     | Esperado |\n",
    "|------|------------------------|----------------------------------------------------------------------------------------------|----------|\n",
    "| 5    | `descricao_caso`       | O texto apresenta os **fatos principais do processo** e **o que foi pedido na aÃ§Ã£o**?        | 1 ou 0   |\n",
    "| 6    | `questoes_em_discussao`| O texto **resume os principais pontos discutidos** no julgamento?                           | 1 ou 0   |\n",
    "| 7    | `tese`                 | O texto apresenta uma **regra geral da decisÃ£o** que pode ser usada em casos semelhantes?   | 1 ou 0   |\n",
    "| 8    | `solucoes_propostas`   | O texto explica, de forma breve, **os fundamentos usados na decisÃ£o**, focando na soluÃ§Ã£o? | 1 ou 0   |\n",
    "| 9    | `envolve_mei`   | O campo **\"envolve_mei\"** recebeu a resposta correta? | 1 ou 0   |\n",
    "| 10    | `mei_do_processo`   |O campo **\"mei_do_processo\"** recebeu a resposta correta? | 1 ou 0   |\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ EstratÃ©gia TÃ©cnica\n",
    "\n",
    "- Para cada campo, uma **pergunta especÃ­fica** Ã© feita ao modelo GPT-3.5.\n",
    "- O texto correspondente do campo Ã© passado como **contexto** dentro do prompt.\n",
    "- O GPT deve responder **apenas com `\"1\"` ou `\"0\"`**, sem justificativa.\n",
    "- A temperatura Ã© fixada em `0.0` para garantir consistÃªncia e reduzir variaÃ§Ã£o de resposta.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… Esse mÃ©todo permite **validaÃ§Ã£o automatizada com interpretaÃ§Ã£o semÃ¢ntica**, sem depender apenas de regras fixas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "524df27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… AvaliaÃ§Ã£o concluÃ­da.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        dbname=\"PROCESSOS\",\n",
    "        user=\"postgres\",\n",
    "        password=\"admin\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "\n",
    "def avaliar_campos_com_gpt35(ementa, resposta_json):\n",
    "    prompt = f\"\"\"\n",
    "VocÃª Ã© um criterioso avaliador de respostas geradas por um modelo de linguagem para ementas judiciais.\n",
    "\n",
    "ReceberÃ¡ a ementa original e a resposta em JSON gerada por outro modelo. Sua tarefa Ã© avaliar se cada campo da resposta estÃ¡ correto, comparando com a ementa.\n",
    "\n",
    "Avalie e retorne APENAS este JSON com \"1\" (correto) ou \"0\" (incorreto) para cada campo:\n",
    "\n",
    "{{\n",
    "  \"nota5\": ...,\n",
    "  \"nota6\": ...,\n",
    "  \"nota7\": ...,\n",
    "  \"nota8\": ...,\n",
    "  \"nota9\": ...,\n",
    "  \"nota10\": ...\n",
    "}}\n",
    "\n",
    "CritÃ©rios:\n",
    "- \"nota5\": O campo \"descricao_caso\" apresenta os fatos principais do processo e o que foi pedido?\n",
    "- \"nota6\": O campo \"questoes_em_discussao\" resume os pontos centrais discutidos no julgamento?\n",
    "- \"nota7\": O campo \"tese\" traz uma regra geral da decisÃ£o ou informa que nÃ£o foi possÃ­vel extrair a tese?\n",
    "- \"nota8\": O campo \"solucoes_propostas\" explica os fundamentos usados com foco na soluÃ§Ã£o do problema?\n",
    "- \"nota9\": O campo \"envolve_mei\" recebeu a resposta correta?\n",
    "- \"nota10\": O campo \"mei_do_processo\" recebeu a resposta correta?\n",
    "\n",
    "### EMENTA ###\n",
    "{ementa}\n",
    "\n",
    "### RESPOSTA ###\n",
    "{resposta_json}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(prompt)\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        resultado = response.choices[0].message.content.strip()\n",
    "        print(\"ðŸ” Resposta da API:\", resultado)\n",
    "        notas = json.loads(resultado)\n",
    "        return (\n",
    "            int(notas.get(\"nota5\", 0)),\n",
    "            int(notas.get(\"nota6\", 0)),\n",
    "            int(notas.get(\"nota7\", 0)),\n",
    "            int(notas.get(\"nota8\", 0)),            \n",
    "            int(notas.get(\"nota9\", 0)),            \n",
    "            int(notas.get(\"nota10\", 0))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao avaliar: {e}\")\n",
    "        return 0, 0, 0, 0\n",
    "\n",
    "# Conectar e carregar os dados\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(\"SELECT id, ementa_completa, resposta AS resposta FROM avaliacoes_sumarizacao where nota5 is null\", conn)\n",
    "\n",
    "# Avaliar e atualizar no banco\n",
    "total = len(df)\n",
    "for idx, row in df.iterrows():\n",
    "    id_registro = row[\"id\"]\n",
    "    ementa = row[\"ementa_completa\"]\n",
    "    resposta_json = row[\"resposta\"]\n",
    "\n",
    "    print(f\"\\n Avaliando processo {idx + 1} de {total} (ID: {id_registro})\")\n",
    "    nota5, nota6, nota7, nota8, nota9, nota10 = avaliar_campos_com_gpt35(ementa, resposta_json)\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        UPDATE avaliacoes_sumarizacao\n",
    "        SET nota5 = %s, nota6 = %s, nota7 = %s, nota8 = %s, nota9 = %s, nota10 = %s\n",
    "        WHERE id = %s\n",
    "    \"\"\", (nota5, nota6, nota7, nota8, nota9, nota10, id_registro))\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    clear_output(wait=True)\n",
    "conn.close()\n",
    "print(\"\\nâœ… AvaliaÃ§Ã£o concluÃ­da.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2dc2c",
   "metadata": {},
   "source": [
    "## RESULTADO\n",
    "\n",
    "## ðŸŽ¯ CritÃ©rios de AvaliaÃ§Ã£o\n",
    "\n",
    "| Nota | CritÃ©rio                                                                                   | Valor Esperado     | PontuaÃ§Ã£o |\n",
    "|------|---------------------------------------------------------------------------------------------|--------------------|-----------|\n",
    "| 1    | O campo `resumo` contÃ©m um JSON bem formado (vÃ¡lido no `json.loads`)                        | JSON vÃ¡lido        | 1 ou 0    |\n",
    "| 2    | O campo `\"decisao\"` da resposta contÃ©m apenas `\"Aceito\"` ou `\"Negado\"`                     | \"Aceito\" / \"Negado\"| 1 ou 0    |\n",
    "| 3    | O campo `\"envolve_mei\"` contÃ©m apenas `\"Sim\"` ou `\"NÃ£o\"`                                    | \"Sim\" / \"NÃ£o\"      | 1 ou 0    |\n",
    "| 4    | O campo `\"mei_do_processo\"` contÃ©m apenas `\"Autor\"`, `\"RÃ©u\"` ou `\"NinguÃ©m\"`                | `\"Autor\"`, `\"RÃ©u\"` ou `\"NinguÃ©m\"`            | 1 ou 0    |\n",
    "| 5    | O campo `\"descricao_caso\"` apresenta os **fatos principais do processo** e **o que foi pedido na aÃ§Ã£o** | -                  | 1 ou 0    |\n",
    "| 6    | O campo `\"questoes_em_discussao\"` resume os **principais pontos discutidos** no julgamento | -                  | 1 ou 0    |\n",
    "| 7    | O campo `\"tese\"` apresenta uma **regra geral da decisÃ£o** aplicÃ¡vel a casos semelhantes     | -                  | 1 ou 0    |\n",
    "| 8    | O campo `\"solucoes_propostas\"` explica, de forma breve, **os fundamentos usados na decisÃ£o**, focando na soluÃ§Ã£o | - | 1 ou 0    |\n",
    "| 9    | O campo `\"envolve_mei\"` recebeu a resposta correta (Sim/NÃ£o) de acordo com o conteÃºdo da ementa | -               | 1 ou 0    |\n",
    "| 10   | O campo `\"mei_do_processo\"` recebeu a resposta correta (Autor/RÃ©u/NinguÃ©m) conforme o processo | -               | 1 ou 0    |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4a94ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Desempenho dos Modelos por Categoria:\n",
      "                         modelo  nota_formato  nota_conteudo  nota_total\n",
      "3                   pixtral-12b           792           1176        1968\n",
      "0                 codestral-22b           776           1170        1946\n",
      "1  deepseek-r1-distill-qwen-14b           719           1146        1865\n",
      "2         llama-3.1-8B-instruct           799           1050        1849\n",
      "\n",
      "ðŸ“‹ Detalhamento por nota:\n",
      "                         modelo  soma_nota1  soma_nota2  soma_nota3  \\\n",
      "0                 codestral-22b         200         183         200   \n",
      "1  deepseek-r1-distill-qwen-14b         200         193         177   \n",
      "2         llama-3.1-8B-instruct         200         200         200   \n",
      "3                   pixtral-12b         200         194         200   \n",
      "\n",
      "   soma_nota4  soma_nota5  soma_nota6  soma_nota7  soma_nota8  soma_nota9  \\\n",
      "0         193         199         199         195         194         198   \n",
      "1         149         197         197         198         196         183   \n",
      "2         199         179         178         176         176         178   \n",
      "3         198         198         198         197         198         194   \n",
      "\n",
      "   soma_nota10  nota_formato  nota_conteudo  nota_total  \n",
      "0          185           776           1170        1946  \n",
      "1          175           719           1146        1865  \n",
      "2          163           799           1050        1849  \n",
      "3          191           792           1176        1968  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loreane\\AppData\\Local\\Temp\\ipykernel_13752\\2057984196.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    }
   ],
   "source": [
    "# Query base\n",
    "query = \"\"\"\n",
    "    SELECT modelo,\n",
    "           SUM(nota1) AS soma_nota1,\n",
    "           SUM(nota2) AS soma_nota2,\n",
    "           SUM(nota3) AS soma_nota3,\n",
    "           SUM(nota4) AS soma_nota4,\n",
    "           SUM(nota5) AS soma_nota5,\n",
    "           SUM(nota6) AS soma_nota6,\n",
    "           SUM(nota7) AS soma_nota7,\n",
    "           SUM(nota8) AS soma_nota8,\n",
    "           SUM(nota9) AS soma_nota9,\n",
    "           SUM(nota10) AS soma_nota10\n",
    "    FROM avaliacoes_sumarizacao\n",
    "    GROUP BY modelo\n",
    "    ORDER BY modelo\n",
    "\"\"\"\n",
    "\n",
    "# Carregar dados\n",
    "conn = get_connection()\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Agrupamentos\n",
    "df['nota_formato'] = df[['soma_nota1', 'soma_nota2', 'soma_nota3', 'soma_nota4']].sum(axis=1)\n",
    "df['nota_conteudo'] = df[['soma_nota5', 'soma_nota6', 'soma_nota7', 'soma_nota8', 'soma_nota9', 'soma_nota10']].sum(axis=1)\n",
    "df['nota_total'] = df['nota_formato'] + df['nota_conteudo']\n",
    "\n",
    "# Exibir resumo geral\n",
    "print(\"ðŸ“Š Desempenho dos Modelos por Categoria:\")\n",
    "print(df[['modelo', 'nota_formato', 'nota_conteudo', 'nota_total']].sort_values(by='nota_total', ascending=False))\n",
    "\n",
    "# Exibir detalhamento completo (nota a nota)\n",
    "print(\"\\nðŸ“‹ Detalhamento por nota:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94d960c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>soma_nota1</th>\n",
       "      <th>soma_nota2</th>\n",
       "      <th>soma_nota3</th>\n",
       "      <th>soma_nota4</th>\n",
       "      <th>soma_nota5</th>\n",
       "      <th>soma_nota6</th>\n",
       "      <th>soma_nota7</th>\n",
       "      <th>soma_nota8</th>\n",
       "      <th>soma_nota9</th>\n",
       "      <th>soma_nota10</th>\n",
       "      <th>nota_formato</th>\n",
       "      <th>nota_conteudo</th>\n",
       "      <th>nota_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codestral-22b</td>\n",
       "      <td>200</td>\n",
       "      <td>183</td>\n",
       "      <td>200</td>\n",
       "      <td>193</td>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "      <td>198</td>\n",
       "      <td>185</td>\n",
       "      <td>776</td>\n",
       "      <td>1170</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deepseek-r1-distill-qwen-14b</td>\n",
       "      <td>200</td>\n",
       "      <td>193</td>\n",
       "      <td>177</td>\n",
       "      <td>149</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>196</td>\n",
       "      <td>183</td>\n",
       "      <td>175</td>\n",
       "      <td>719</td>\n",
       "      <td>1146</td>\n",
       "      <td>1865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-3.1-8B-instruct</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>176</td>\n",
       "      <td>176</td>\n",
       "      <td>178</td>\n",
       "      <td>163</td>\n",
       "      <td>799</td>\n",
       "      <td>1050</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pixtral-12b</td>\n",
       "      <td>200</td>\n",
       "      <td>194</td>\n",
       "      <td>200</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>194</td>\n",
       "      <td>191</td>\n",
       "      <td>792</td>\n",
       "      <td>1176</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         modelo  soma_nota1  soma_nota2  soma_nota3  \\\n",
       "0                 codestral-22b         200         183         200   \n",
       "1  deepseek-r1-distill-qwen-14b         200         193         177   \n",
       "2         llama-3.1-8B-instruct         200         200         200   \n",
       "3                   pixtral-12b         200         194         200   \n",
       "\n",
       "   soma_nota4  soma_nota5  soma_nota6  soma_nota7  soma_nota8  soma_nota9  \\\n",
       "0         193         199         199         195         194         198   \n",
       "1         149         197         197         198         196         183   \n",
       "2         199         179         178         176         176         178   \n",
       "3         198         198         198         197         198         194   \n",
       "\n",
       "   soma_nota10  nota_formato  nota_conteudo  nota_total  \n",
       "0          185           776           1170        1946  \n",
       "1          175           719           1146        1865  \n",
       "2          163           799           1050        1849  \n",
       "3          191           792           1176        1968  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7ba62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
